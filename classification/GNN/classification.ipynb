{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ab4add",
   "metadata": {},
   "source": [
    "# Predicting Satisfiability of SAT-3 Problems\n",
    "## Solving the Classification Task using a ***Graphical Neural Network (GNN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1811576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836b5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "__counter__ = random.randint(0,2e9)\n",
    "\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f6b006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import json\n",
    "\n",
    "from tuning import tune_parameters\n",
    "from data_loader import dataset_processing\n",
    "from train import training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ed2b0",
   "metadata": {},
   "source": [
    "A helper function to ensure that the dataset is made from scratch when NN is tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af9788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder_contents(folders):\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d6109",
   "metadata": {},
   "source": [
    "### Test set sampled from the same distribution as the training and validation sets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d59a9e",
   "metadata": {},
   "source": [
    "In order to train the model, a *training set* is used alongside with a *validation set*. The latter is used to measure the performance of the model. The training and validation set were extracted from the 80% (60%-20%) of the data and the rest 20% was kept to evaluate the model after its training (*test set*).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be0d4d",
   "metadata": {},
   "source": [
    "Erase the previous dataset created by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33653804",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder_contents([\"./raw\", \"./processed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977c44e",
   "metadata": {},
   "source": [
    "**Create the dataset**: The dataset is created from the raw data in such a format that it can be loader by the *DataLoader* module of torch.geometric. <br>\n",
    "\n",
    "The dataset must be provided to Pytorch in a graph format, so in this preprocessing are constructed the *vertices, the edges, the labels* etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "354f0454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the data processing...\n",
      "\n",
      "Satisfiable CNFs   : 3701\n",
      "Unsatisfiable CNFs : 2700\n",
      "\n",
      "Ratio of SAT   : 0.5782\n",
      "Ratio of UNSAT : 0.4218\n",
      "\n",
      "Training set size: 5120\n",
      "Test set size: 1280\n",
      "\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "pos_weight = dataset_processing(separate_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b7dd0",
   "metadata": {},
   "source": [
    "**Tune parameters**: Tune the parameters of the model (GNN itself) as well as parameters regarding the training process e.g. *batch-size*, *weight decay* etc. More specidically, the parameters that are tuned are the following.\n",
    "<ol><li>Model parameters:</li>\n",
    "    <ul> \n",
    "        <li>Number of layers</li>\n",
    "        <li>Dropout rate</li>\n",
    "        <li>Neurons' density in its last Linear Layers</li>\n",
    "        <li>Embedding size for the Graph Transformer Operator (TransformerConv)</li>\n",
    "        <li>Attention heads for the Graph Transformer Operator (TransformerConv)</li><br>\n",
    "    </ul>\n",
    "    <li>Training parameters:</li>\n",
    "    <ul> \n",
    "        <li>Batch size used</li>\n",
    "        <li>Learning rate</li>\n",
    "        <li>Weight decay</li>\n",
    "    </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2055c",
   "metadata": {},
   "source": [
    "The parameters of the final model are the ones that result in the minimum ***validation error***. <br>\n",
    "Also, ***early stopping*** is used in order to avoid *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune parameters\n",
    "best_parameters = tune_parameters(pos_weight=pos_weight)\n",
    "\n",
    "# Show best parameters\n",
    "print(f'Best hyperparameters were: {best_parameters}')\n",
    "# Store best parameters\n",
    "with open('./best_parameters_same_sets.txt', 'w') as f:\n",
    "    f.write(json.dumps(best_parameters))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff887d",
   "metadata": {},
   "source": [
    "**Train with the best parameters**: The GNN is trained using the selected parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cf7a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training with the best parameters\n",
      "\n",
      "Dataset loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5120/5120 [00:07<00:00, 727.86it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading completed\n",
      "\n",
      "Model loading...\n",
      "Model loading completed\n",
      "\n",
      "EPOCH | 0\n",
      "Training Loss   : 0.5869\n",
      "Validation Loss : 0.5740\n",
      "\n",
      "EPOCH | 1\n",
      "Training Loss   : 0.5366\n",
      "Validation Loss : 0.5505\n",
      "\n",
      "EPOCH | 2\n",
      "Training Loss   : 0.4253\n",
      "Validation Loss : 0.4146\n",
      "\n",
      "EPOCH | 3\n",
      "Training Loss   : 0.2238\n",
      "Validation Loss : 0.2393\n",
      "\n",
      "EPOCH | 4\n",
      "Training Loss   : 0.1586\n",
      "Validation Loss : 0.3276\n",
      "\n",
      "EPOCH | 5\n",
      "Training Loss   : 0.1180\n",
      "Validation Loss : 1.0890\n",
      "\n",
      "EPOCH | 6\n",
      "Training Loss   : 0.0865\n",
      "Validation Loss : 0.1049\n",
      "\n",
      "EPOCH | 7\n",
      "Training Loss   : 0.0732\n",
      "Validation Loss : 0.2686\n",
      "\n",
      "EPOCH | 8\n",
      "Training Loss   : 0.0639\n",
      "Validation Loss : 0.1308\n",
      "\n",
      "EPOCH | 9\n",
      "Training Loss   : 0.0464\n",
      "Validation Loss : 0.2120\n",
      "\n",
      "EPOCH | 10\n",
      "Training Loss   : 0.0412\n",
      "Validation Loss : 0.0851\n",
      "\n",
      "EPOCH | 11\n",
      "Training Loss   : 0.0332\n",
      "Validation Loss : 0.0170\n",
      "\n",
      "EPOCH | 12\n",
      "Training Loss   : 0.0186\n",
      "Validation Loss : 0.0098\n",
      "\n",
      "EPOCH | 13\n",
      "Training Loss   : 0.0143\n",
      "Validation Loss : 0.0686\n",
      "\n",
      "EPOCH | 14\n",
      "Training Loss   : 0.0110\n",
      "Validation Loss : 0.1842\n",
      "\n",
      "EPOCH | 15\n",
      "Training Loss   : 0.0094\n",
      "Validation Loss : 0.2161\n",
      "\n",
      "EPOCH | 16\n",
      "Training Loss   : 0.0073\n",
      "Validation Loss : 0.0053\n",
      "\n",
      "EPOCH | 17\n",
      "Training Loss   : 0.0060\n",
      "Validation Loss : 0.0052\n",
      "\n",
      "EPOCH | 18\n",
      "Training Loss   : 0.0048\n",
      "Validation Loss : 0.0078\n",
      "\n",
      "EPOCH | 19\n",
      "Training Loss   : 0.0043\n",
      "Validation Loss : 0.0055\n",
      "\n",
      "EPOCH | 20\n",
      "Training Loss   : 0.0035\n",
      "Validation Loss : 0.0047\n",
      "\n",
      "EPOCH | 21\n",
      "Training Loss   : 0.0028\n",
      "Validation Loss : 0.0068\n",
      "\n",
      "EPOCH | 22\n",
      "Training Loss   : 0.0025\n",
      "Validation Loss : 0.0051\n",
      "\n",
      "EPOCH | 23\n",
      "Training Loss   : 0.0022\n",
      "Validation Loss : 0.0054\n",
      "\n",
      "EPOCH | 24\n",
      "Training Loss   : 0.0021\n",
      "Validation Loss : 0.0056\n",
      "\n",
      "EPOCH | 25\n",
      "Training Loss   : 0.0019\n",
      "Validation Loss : 0.0067\n",
      "\n",
      "EPOCH | 26\n",
      "Training Loss   : 0.0018\n",
      "Validation Loss : 0.0064\n",
      "\n",
      "EPOCH | 27\n",
      "Training Loss   : 0.0017\n",
      "Validation Loss : 0.0055\n",
      "\n",
      "EPOCH | 28\n",
      "Training Loss   : 0.0016\n",
      "Validation Loss : 0.0052\n",
      "\n",
      "EPOCH | 29\n",
      "Training Loss   : 0.0015\n",
      "Validation Loss : 0.0051\n",
      "\n",
      "EPOCH | 30\n",
      "Training Loss   : 0.0015\n",
      "Validation Loss : 0.0045\n",
      "\n",
      "EPOCH | 31\n",
      "Training Loss   : 0.0014\n",
      "Validation Loss : 0.0040\n",
      "\n",
      "EPOCH | 32\n",
      "Training Loss   : 0.0014\n",
      "Validation Loss : 0.0037\n",
      "\n",
      "EPOCH | 33\n",
      "Early stopping activated, with training and validation loss difference: 0.0008\n",
      "\n",
      "Training has stopped, now continuing with logging\n",
      "\n",
      "EPOCH | 34\n",
      "Training Loss   : 0.0013\n",
      "Validation Loss : 0.0034\n",
      "\n",
      "EPOCH | 35\n",
      "Training Loss   : 0.0013\n",
      "Validation Loss : 0.0031\n",
      "\n",
      "EPOCH | 36\n",
      "Training Loss   : 0.0012\n",
      "Validation Loss : 0.0030\n",
      "\n",
      "EPOCH | 37\n",
      "Training Loss   : 0.0012\n",
      "Validation Loss : 0.0029\n",
      "\n",
      "EPOCH | 38\n",
      "Training Loss   : 0.0012\n",
      "Validation Loss : 0.0027\n",
      "\n",
      "EPOCH | 39\n",
      "Training Loss   : 0.0012\n",
      "Validation Loss : 0.0027\n",
      "\n",
      "EPOCH | 40\n",
      "Training Loss   : 0.0011\n",
      "Validation Loss : 0.0026\n",
      "\n",
      "EPOCH | 41\n",
      "Training Loss   : 0.0011\n",
      "Validation Loss : 0.0025\n",
      "\n",
      "EPOCH | 42\n",
      "Training Loss   : 0.0011\n",
      "Validation Loss : 0.0025\n",
      "\n",
      "EPOCH | 43\n",
      "Training Loss   : 0.0011\n",
      "Validation Loss : 0.0024\n",
      "\n",
      "EPOCH | 44\n",
      "Training Loss   : 0.0011\n",
      "Validation Loss : 0.0024\n",
      "\n",
      "EPOCH | 45\n",
      "Training Loss   : 0.0011\n",
      "Validation Loss : 0.0023\n",
      "\n",
      "EPOCH | 46\n",
      "Training Loss   : 0.0010\n",
      "Validation Loss : 0.0023\n",
      "\n",
      "EPOCH | 47\n",
      "Training Loss   : 0.0010\n",
      "Validation Loss : 0.0023\n",
      "\n",
      "EPOCH | 48\n",
      "Training Loss   : 0.0010\n",
      "Validation Loss : 0.0022\n",
      "\n",
      "EPOCH | 49\n",
      "Training Loss   : 0.0010\n",
      "Validation Loss : 0.0022\n",
      "\n",
      "EPOCH | 50\n",
      "Training Loss   : 0.0010\n",
      "Validation Loss : 0.0022\n",
      "\n",
      "Finishing training with best training loss: 0.0060 and best validation loss: 0.0052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0052026261037099175"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the best parameters in order to train final model\n",
    "with open('./best_parameters_same_sets.txt') as f:\n",
    "    data = f.read()\n",
    "\n",
    "best_parameters_loaded = json.loads(data)\n",
    "\n",
    "print('\\nNow training with the best parameters\\n')\n",
    "training(best_parameters_loaded, make_err_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180905a7",
   "metadata": {},
   "source": [
    "The following Figure shows the ***training and validation set errors***, the epoch where the ***early stopping*** was activated, as well as the epoch of the final ***selected model***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef04a73b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m display(\u001b[43mHTML\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<img src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplots/train_valid_error.png?\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m height=400 width=400>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m __counter__))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "display(HTML('<img src=\"plots/train_valid_error.png?%d\" height=400 width=400>' % __counter__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64fc27",
   "metadata": {},
   "source": [
    "**Test the model**: Predict the satisfiability of the clauses in the *Test Set* and get the corresponding metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87ae1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on the test set:\n",
      "\n",
      "Dataset loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:01<00:00, 724.32it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading completed\n",
      "\n",
      "Model loading...\n",
      "Model loading completed\n",
      "\n",
      "\n",
      "Test set metrics:\n",
      "\n",
      " Confusion matrix: \n",
      " [[521   0]\n",
      " [  0 759]]\n",
      "F1 Score  : 1.0000\n",
      "Accuracy  : 1.0000\n",
      "Precision : 1.0000\n",
      "Recall    : 1.0000\n",
      "ROC AUC   : 1.0000\n",
      "Test Loss : 0.0036169828461424915\n"
     ]
    }
   ],
   "source": [
    "print('\\nResults on the test set:\\n')\n",
    "testing(params=best_parameters_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb56bf",
   "metadata": {},
   "source": [
    "The following **Figures** show:\n",
    "<ol>\n",
    "<li>The <b>confusion matrix</b> for the test set-prediction</li>\n",
    "<li>The <b>ROC-AUC curve</b> for the test set-prediction</li>\n",
    "<li>The <b>precision recall curve</b> for the test set-prediction</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d64565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"plots/cm.png?420024730\" height=500 width=500>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"plots/roc_auc.png?420024730\" height=450 width=450>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"plots/pr.png?420024730\" height=450 width=450>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n1.\")\n",
    "display(HTML('<img src=\"plots/cm.png?%d\" height=500 width=500>' % __counter__))\n",
    "print(\"2.\")\n",
    "display(HTML('<img src=\"plots/roc_auc.png?%d\" height=450 width=450>' % __counter__))\n",
    "print(\"3.\")\n",
    "display(HTML('<img src=\"plots/pr.png?%d\" height=450 width=450>' % __counter__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a56f2",
   "metadata": {},
   "source": [
    "### Test set sampled from a different distribution from the training and validation sets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ded7b",
   "metadata": {},
   "source": [
    "Here, again, in order to train the model, a training set is used alongside with a validation set. However, now, they are both sampled randomly from the smaller 3-SAT problems i.e. have less than 250 variables. The test set consists of 200 problem instances that have 250 variables and 1065 clauses each. Since in this case we would like to predict the satisfiability of the problems that the well known algorithms cannot predict, ***we want to measure the performance of the model for bigger instances that it has not seen during its training***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b2456",
   "metadata": {},
   "source": [
    "The training-tuning process is the same as showcased above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ada91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erase the previous dataset created by PyTorch\n",
    "delete_folder_contents([\"./raw\", \"./processed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b30c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the data processing...\n",
      "\n",
      "Satisfiable CNFs   : 3601\n",
      "Unsatisfiable CNFs : 2600\n",
      "\n",
      "Ratio of SAT   : 0.5807\n",
      "Ratio of UNSAT : 0.4193\n",
      "\n",
      "Training set size: 6200\n",
      "Test set size: 200\n",
      "\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "pos_weight = dataset_processing(separate_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b944bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune parameters\n",
    "best_parameters = tune_parameters(pos_weight=pos_weight)\n",
    "\n",
    "# Show best parameters\n",
    "print(f'Best hyperparameters were: {best_parameters}')\n",
    "# Store best parameters\n",
    "with open('best_parameters_diff_test.txt', 'w') as f:\n",
    "    f.write(json.dumps(best_parameters))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d534af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training with the best parameters\n",
      "\n",
      "Dataset loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6200/6200 [00:08<00:00, 754.26it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading completed\n",
      "\n",
      "Model loading...\n",
      "Model loading completed\n",
      "\n",
      "EPOCH | 0\n",
      "Training Loss   : 0.5707\n",
      "Validation Loss : 0.6077\n",
      "\n",
      "EPOCH | 1\n",
      "Training Loss   : 0.4637\n",
      "Validation Loss : 1.2736\n",
      "\n",
      "EPOCH | 2\n",
      "Training Loss   : 0.3474\n",
      "Validation Loss : 0.3346\n",
      "\n",
      "EPOCH | 3\n",
      "Training Loss   : 0.2451\n",
      "Validation Loss : 0.1784\n",
      "\n",
      "EPOCH | 4\n",
      "Training Loss   : 0.1733\n",
      "Validation Loss : 0.3987\n",
      "\n",
      "EPOCH | 5\n",
      "Training Loss   : 0.1499\n",
      "Validation Loss : 0.1641\n",
      "\n",
      "EPOCH | 6\n",
      "Training Loss   : 0.1296\n",
      "Validation Loss : 0.2813\n",
      "\n",
      "EPOCH | 7\n",
      "Training Loss   : 0.1241\n",
      "Validation Loss : 0.2997\n",
      "\n",
      "EPOCH | 8\n",
      "Training Loss   : 0.1294\n",
      "Validation Loss : 0.1490\n",
      "\n",
      "EPOCH | 9\n",
      "Training Loss   : 0.1064\n",
      "Validation Loss : 0.1292\n",
      "\n",
      "EPOCH | 10\n",
      "Training Loss   : 0.1062\n",
      "Validation Loss : 0.0969\n",
      "\n",
      "EPOCH | 11\n",
      "Training Loss   : 0.1005\n",
      "Validation Loss : 0.1055\n",
      "\n",
      "EPOCH | 12\n",
      "Training Loss   : 0.0953\n",
      "Validation Loss : 0.1565\n",
      "\n",
      "EPOCH | 13\n",
      "Training Loss   : 0.0927\n",
      "Validation Loss : 0.1870\n",
      "\n",
      "EPOCH | 14\n",
      "Training Loss   : 0.0879\n",
      "Validation Loss : 0.2328\n",
      "\n",
      "EPOCH | 15\n",
      "Training Loss   : 0.0798\n",
      "Validation Loss : 0.2980\n",
      "\n",
      "EPOCH | 16\n",
      "Training Loss   : 0.0755\n",
      "Validation Loss : 0.4415\n",
      "\n",
      "EPOCH | 17\n",
      "Training Loss   : 0.0701\n",
      "Validation Loss : 0.7117\n",
      "\n",
      "EPOCH | 18\n",
      "Training Loss   : 0.0674\n",
      "Validation Loss : 1.1335\n",
      "\n",
      "EPOCH | 19\n",
      "Training Loss   : 0.0608\n",
      "Validation Loss : 1.8069\n",
      "\n",
      "EPOCH | 20\n",
      "Training Loss   : 0.0550\n",
      "Validation Loss : 2.5723\n",
      "\n",
      "EPOCH | 21\n",
      "Training Loss   : 0.0498\n",
      "Validation Loss : 2.5556\n",
      "\n",
      "EPOCH | 22\n",
      "Training Loss   : 0.0442\n",
      "Validation Loss : 2.5298\n",
      "\n",
      "EPOCH | 23\n",
      "Training Loss   : 0.0398\n",
      "Validation Loss : 2.7772\n",
      "\n",
      "EPOCH | 24\n",
      "Training Loss   : 0.0365\n",
      "Validation Loss : 3.0050\n",
      "\n",
      "EPOCH | 25\n",
      "Training Loss   : 0.0313\n",
      "Validation Loss : 2.9385\n",
      "\n",
      "EPOCH | 26\n",
      "Training Loss   : 0.0273\n",
      "Validation Loss : 2.9980\n",
      "\n",
      "EPOCH | 27\n",
      "Early stopping activated, with training and validation loss difference: 0.0050\n",
      "\n",
      "Training has stopped, now continuing with logging\n",
      "\n",
      "EPOCH | 28\n",
      "Training Loss   : 0.0231\n",
      "Validation Loss : 3.2960\n",
      "\n",
      "EPOCH | 29\n",
      "Training Loss   : 0.0200\n",
      "Validation Loss : 3.3751\n",
      "\n",
      "EPOCH | 30\n",
      "Training Loss   : 0.0176\n",
      "Validation Loss : 3.5413\n",
      "\n",
      "EPOCH | 31\n",
      "Training Loss   : 0.0159\n",
      "Validation Loss : 3.8489\n",
      "\n",
      "EPOCH | 32\n",
      "Training Loss   : 0.0146\n",
      "Validation Loss : 4.0354\n",
      "\n",
      "EPOCH | 33\n",
      "Training Loss   : 0.0135\n",
      "Validation Loss : 3.9456\n",
      "\n",
      "EPOCH | 34\n",
      "Training Loss   : 0.0121\n",
      "Validation Loss : 3.7723\n",
      "\n",
      "EPOCH | 35\n",
      "Training Loss   : 0.0115\n",
      "Validation Loss : 3.8813\n",
      "\n",
      "EPOCH | 36\n",
      "Training Loss   : 0.0111\n",
      "Validation Loss : 3.8459\n",
      "\n",
      "EPOCH | 37\n",
      "Training Loss   : 0.0107\n",
      "Validation Loss : 3.9341\n",
      "\n",
      "EPOCH | 38\n",
      "Training Loss   : 0.0099\n",
      "Validation Loss : 3.9413\n",
      "\n",
      "EPOCH | 39\n",
      "Training Loss   : 0.0092\n",
      "Validation Loss : 3.9857\n",
      "\n",
      "EPOCH | 40\n",
      "Training Loss   : 0.0086\n",
      "Validation Loss : 4.0413\n",
      "\n",
      "EPOCH | 41\n",
      "Training Loss   : 0.0080\n",
      "Validation Loss : 4.1312\n",
      "\n",
      "EPOCH | 42\n",
      "Training Loss   : 0.0076\n",
      "Validation Loss : 4.2153\n",
      "\n",
      "EPOCH | 43\n",
      "Training Loss   : 0.0072\n",
      "Validation Loss : 4.2589\n",
      "\n",
      "EPOCH | 44\n",
      "Training Loss   : 0.0068\n",
      "Validation Loss : 4.3064\n",
      "\n",
      "EPOCH | 45\n",
      "Training Loss   : 0.0065\n",
      "Validation Loss : 4.3560\n",
      "\n",
      "EPOCH | 46\n",
      "Training Loss   : 0.0062\n",
      "Validation Loss : 4.3885\n",
      "\n",
      "EPOCH | 47\n",
      "Training Loss   : 0.0060\n",
      "Validation Loss : 4.4272\n",
      "\n",
      "EPOCH | 48\n",
      "Training Loss   : 0.0058\n",
      "Validation Loss : 4.4465\n",
      "\n",
      "EPOCH | 49\n",
      "Training Loss   : 0.0056\n",
      "Validation Loss : 4.4863\n",
      "\n",
      "EPOCH | 50\n",
      "Training Loss   : 0.0054\n",
      "Validation Loss : 4.5036\n",
      "\n",
      "Finishing training with best training loss: 0.1005 and best validation loss: 0.1055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10546847768366718"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with the best parameters\n",
    "\n",
    "# Access the best parameters in order to train final model\n",
    "with open('best_parameters_diff_test.txt') as f:\n",
    "    data = f.read()\n",
    "\n",
    "best_parameters_loaded = json.loads(data)\n",
    "\n",
    "print('\\nNow training with the best parameters\\n')\n",
    "training(best_parameters_loaded, make_err_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ba08856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on the test set:\n",
      "\n",
      "Dataset loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 423.13it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading completed\n",
      "\n",
      "Model loading...\n",
      "Model loading completed\n",
      "\n",
      "\n",
      "Test set metrics:\n",
      "\n",
      " Confusion matrix: \n",
      " [[  0   0]\n",
      " [100 100]]\n",
      "F1 Score  : 0.6667\n",
      "Accuracy  : 0.5000\n",
      "Precision : 0.5000\n",
      "Recall    : 1.0000\n",
      "ROC AUC   : 0.5000\n",
      "Test Loss : 2.3636052792093585\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "print('\\nResults on the test set:\\n')\n",
    "testing(params=best_parameters_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41dd07",
   "metadata": {},
   "source": [
    "The following **Figures** show:\n",
    "<ol>\n",
    "<li>The <b>confusion matrix</b> for the test set-prediction</li>\n",
    "<li>The <b>ROC-AUC curve</b> for the test set-prediction</li>\n",
    "<li>The <b>precision recall curve</b> for the test set-prediction</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d851b3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"plots/cm.png?522265296\" height=500 width=500>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"plots/roc_auc.png?522265296\" height=450 width=450>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"plots/pr.png?522265296\" height=450 width=450>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n1.\")\n",
    "display(HTML('<img src=\"plots/cm.png?%d\" height=500 width=500>' % __counter__))\n",
    "print(\"2.\")\n",
    "display(HTML('<img src=\"plots/roc_auc.png?%d\" height=450 width=450>' % __counter__))\n",
    "print(\"3.\")\n",
    "display(HTML('<img src=\"plots/pr.png?%d\" height=450 width=450>' % __counter__))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepLearningNew)",
   "language": "python",
   "name": "deeplearningnew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
