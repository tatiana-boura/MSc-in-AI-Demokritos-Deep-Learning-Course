{"batch_size": 16, "learning_rate": 0.001, "weight_decay": 0.0001, "pos_weight": 1.0, "model_embedding_size": 64, "model_attention_heads": 1, "model_layers": 2, "model_dropout_rate": 0.1, "model_dense_neurons": 256}